{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df9452a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import argparse\n",
    "import cv2\n",
    "from scipy import io\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from modules import utils\n",
    "from modules.models import INR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7de91a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='INCODE')\n",
    "\n",
    "# Shared Parameters\n",
    "parser.add_argument('--input',type=str, default='./incode_data/Image/celtic_spiral_knot.jpg', help='Input image path')\n",
    "parser.add_argument('--inr_model',type=str, default='parac', help='[gauss, mfn, relu, siren, wire, wire2d, ffn, incode, parac]')\n",
    "parser.add_argument('--lr',type=float, default=1.5e-4, help='Learning rate')\n",
    "parser.add_argument('--using_schedular', type=bool, default=True, help='Whether to use schedular')\n",
    "parser.add_argument('--scheduler_b', type=float, default=0.25, help='Learning rate scheduler')\n",
    "parser.add_argument('--maxpoints', type=int, default=8192, help='Batch size')\n",
    "parser.add_argument('--niters', type=int, default=501, help='Number if iterations')\n",
    "parser.add_argument('--steps_til_summary', type=int, default=20, help='Number of steps till summary visualization')\n",
    "parser.add_argument('--sampling_ratio', type=float, default=0.2, help='The percentage of pixels used for training')\n",
    "\n",
    "\n",
    "# INCODE Parameters\n",
    "parser.add_argument('--a_coef',type=float, default=0.1993, help='a coeficient')\n",
    "parser.add_argument('--b_coef',type=float, default=0.0196, help='b coeficient')\n",
    "parser.add_argument('--c_coef',type=float, default=0.0588, help='c coeficient')\n",
    "parser.add_argument('--d_coef',type=float, default=0.0269, help='d coeficient')\n",
    "\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c268067e",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5d16c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = np.array(Image.open(args.input))\n",
    "im = torch.from_numpy(im).float()/255.0\n",
    "\n",
    "H, W, C = im.shape\n",
    "pixel_count = H * W\n",
    "\n",
    "\n",
    "sampled_pixel_count = int(pixel_count * args.sampling_ratio)\n",
    "\n",
    "img_mask, img_train = utils.build_train_data(im, sampled_pixel_count)\n",
    "train_dataset = TensorDataset(img_mask, img_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=args.maxpoints, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ab2563",
   "metadata": {},
   "source": [
    "## Defining Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c717b7",
   "metadata": {},
   "source": [
    "### Defining desired Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64cdf32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency Encoding\n",
    "pos_encode_freq = {'type':'frequency', 'use_nyquist': True, 'mapping_input': sampled_pixel_count}\n",
    "\n",
    "# Gaussian Encoding\n",
    "pos_encode_gaus = {'type':'gaussian', 'scale_B': 10, 'mapping_input': 256}\n",
    "\n",
    "# No Encoding\n",
    "pos_encode_no = {'type': None}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c1145d",
   "metadata": {},
   "source": [
    "### Model Configureations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22d602b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Harmonizer Configurations\n",
    "# MLP_configs={'task': 'inpainting',\n",
    "#              'in_channels': 64,             \n",
    "#              'hidden_channels': [64, 32, 4],\n",
    "#              'mlp_bias':0.3120,\n",
    "#              'activation_layer': nn.SiLU,\n",
    "#              'GT': img_train[None, ...].permute(0, 2, 1).to(device)\n",
    "#             }\n",
    "\n",
    "# ### Model Configurations\n",
    "# model = INR(args.inr_model).run(in_features=2,\n",
    "#                                 out_features=3, \n",
    "#                                 hidden_features=256,\n",
    "#                                 hidden_layers=3,\n",
    "#                                 first_omega_0=30.0,\n",
    "#                                 hidden_omega_0=30.0,\n",
    "#                                 pos_encode_configs=pos_encode_no, \n",
    "#                                 MLP_configs = MLP_configs\n",
    "#                                ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b42f150f-b7ab-484f-9e4e-b0924cb3407a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model Configurations for parac\n",
    "model = INR(args.inr_model).run(in_features=2,\n",
    "                                out_features=3, \n",
    "                                hidden_features=256,\n",
    "                                hidden_layers=3,\n",
    "                                first_omega_0=30.0,\n",
    "                                hidden_omega_0=30.0\n",
    "                               ).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cca532e",
   "metadata": {},
   "source": [
    "# Testing code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b680158",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_model, b_size=args.maxpoints):\n",
    "    \n",
    "    # Data processing\n",
    "    img_mask, img_eval = utils.build_eval_data(im)\n",
    "    test_dataset = TensorDataset(img_mask, img_eval)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=b_size, shuffle=False, pin_memory=True)\n",
    "    \n",
    "    # test\n",
    "    with torch.no_grad():\n",
    "        iterator = test_dataloader\n",
    "        predictions = []\n",
    "\n",
    "        for batch in iterator:\n",
    "            inputs, _ = batch\n",
    "\n",
    "            with torch.no_grad():\n",
    "                prediction, _ = test_model(inputs.cuda())\n",
    "\n",
    "            predictions.append(prediction)\n",
    "\n",
    "        predicted_image = torch.cat(predictions).cpu().numpy()\n",
    "        predicted_image = predicted_image.reshape((H, W, C)).astype(np.float32)\n",
    "\n",
    "        return predicted_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db268c3d",
   "metadata": {},
   "source": [
    "## Training Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f58b062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer setup\n",
    "if args.inr_model == 'wire':\n",
    "    args.lr = args.lr * min(1, args.maxpoints / (H * W))\n",
    "optim = torch.optim.Adam(lr=args.lr, params=model.parameters())\n",
    "scheduler = lr_scheduler.LambdaLR(optim, lambda x: args.scheduler_b ** min(x / args.niters, 1))\n",
    "\n",
    "# Initialize lists for PSNR values\n",
    "psnr_train_values = []\n",
    "psnr_test_values = []\n",
    "\n",
    "# Initialize best loss value as positive infinity\n",
    "best_loss = torch.tensor(float('inf'))\n",
    "\n",
    "# Check the args.maxpoints value\n",
    "args.maxpoints = int(min(len(img_mask), args.maxpoints))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a452e7cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c3e170ee61d489dbf1e57b021efcc0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/501 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape: torch.Size([1, 8192, 2])\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 200.00 MiB. GPU 0 has a total capacity of 1.83 GiB of which 4.88 MiB is free. Process 15851 has 1.71 GiB memory in use. Including non-PyTorch memory, this process has 104.00 MiB memory in use. Of the allocated memory 25.11 MiB is allocated by PyTorch, and 16.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minputs.shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minputs\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m     model_output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Calculate the output loss\u001b[39;00m\n\u001b[1;32m     20\u001b[0m output_loss \u001b[38;5;241m=\u001b[39m ((model_output \u001b[38;5;241m-\u001b[39m targets)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[0;32m~/.virtualenvs/inr/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/inr/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/run/media/neo/joint/compsci/projects/INCODE/modules/paracnet.py:176\u001b[0m, in \u001b[0;36mINR.forward\u001b[0;34m(self, coords)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, coords):\n\u001b[0;32m--> 176\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoords\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/.virtualenvs/inr/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/inr/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenvs/inr/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.virtualenvs/inr/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/inr/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/run/media/neo/joint/compsci/projects/INCODE/modules/paracnet.py:94\u001b[0m, in \u001b[0;36mParasin.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     92\u001b[0m temp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# print(temp.shape)\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_act\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/run/media/neo/joint/compsci/projects/INCODE/modules/paracnet.py:98\u001b[0m, in \u001b[0;36mParasin.param_act\u001b[0;34m(self, linout)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparam_act\u001b[39m(\u001b[38;5;28mself\u001b[39m, linout):\n\u001b[1;32m     97\u001b[0m     ws, bs, phis \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mws, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mphis)\n\u001b[0;32m---> 98\u001b[0m     linoutx \u001b[38;5;241m=\u001b[39m \u001b[43mlinout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat_interleave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mws\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m     wsx \u001b[38;5;241m=\u001b[39m ws\u001b[38;5;241m.\u001b[39mexpand(linout\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], linout\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], linout\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    100\u001b[0m     bsx \u001b[38;5;241m=\u001b[39m bs\u001b[38;5;241m.\u001b[39mexpand(linout\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], linout\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], linout\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 200.00 MiB. GPU 0 has a total capacity of 1.83 GiB of which 4.88 MiB is free. Process 15851 has 1.71 GiB memory in use. Including non-PyTorch memory, this process has 104.00 MiB memory in use. Of the allocated memory 25.11 MiB is allocated by PyTorch, and 16.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "for step in tqdm(range(args.niters)):\n",
    "\n",
    "    loss_values = []\n",
    "    # Process data points in batches\n",
    "    for batch in train_dataloader:\n",
    "        inputs, targets = batch\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Calculate model output\n",
    "        if args.inr_model == 'incode':\n",
    "            print(f'inputs.shape: {inputs.shape}')\n",
    "            model_output, coef = model(inputs)  \n",
    "        else:\n",
    "            inputs = inputs.unsqueeze(0)\n",
    "            print(f'inputs.shape: {inputs.shape}')\n",
    "            \n",
    "            model_output = model(inputs) \n",
    "\n",
    "        # Calculate the output loss\n",
    "        output_loss = ((model_output - targets)**2).mean()\n",
    "        \n",
    "        if args.inr_model == 'incode':\n",
    "            # Calculate regularization loss for 'incode' model\n",
    "            a_coef, b_coef, c_coef, d_coef = coef[0]  \n",
    "            reg_loss = args.a_coef * torch.relu(-a_coef) + \\\n",
    "                       args.b_coef * torch.relu(-b_coef) + \\\n",
    "                       args.c_coef * torch.relu(-c_coef) + \\\n",
    "                       args.d_coef * torch.relu(-d_coef)\n",
    "\n",
    "            # Total loss for 'incode' model\n",
    "            loss = output_loss + reg_loss \n",
    "        else: \n",
    "            # Total loss for other models\n",
    "            loss = output_loss\n",
    "        \n",
    "        loss_values.append(loss.item())\n",
    "\n",
    "        # Perform backpropagation and update model parameters\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    \n",
    "    avg_loss = np.mean(loss_values)\n",
    "\n",
    "    # Calculate PSNR\n",
    "    with torch.no_grad():\n",
    "        psnr = -10*torch.log10(((model_output - targets)**2).mean())\n",
    "        psnr_train_values.append(psnr.item())\n",
    "\n",
    "    # Adjust learning rate using a scheduler if applicable\n",
    "    if args.using_schedular:\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "    # Display intermediate results at specified intervals\n",
    "    if step % args.steps_til_summary == 0:\n",
    "        best_img = test(model)        \n",
    "        mse_loss_array = ((im - best_img)**2).mean().item()\n",
    "        psnr_test = -10*np.log10(mse_loss_array)\n",
    "        psnr_test_values.append(psnr_test.item())\n",
    "        \n",
    "        best_img = (best_img - best_img.min()) / (best_img.max() - best_img.min())\n",
    "\n",
    "        print(\"Epoch: {} | Total Loss: {:.5f} | PSNR: {:.4f} | Test PSNR: {:.4f}\".format(step, \n",
    "                                                                                avg_loss,\n",
    "                                                                                psnr.item(),\n",
    "                                                                                psnr_test.item()))\n",
    "                \n",
    "        # Plot\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(6, 6))\n",
    "        axes[0].set_title('Ground Truth')\n",
    "        axes[0].imshow(im)\n",
    "        axes[0].axis('off')\n",
    "        axes[1].set_title('Output')\n",
    "        axes[1].imshow(best_img)\n",
    "        axes[1].axis('off')\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "# Print maximum PSNR achieved during training\n",
    "print('--------------------')\n",
    "print('Train Max PSNR:', max(psnr_train_values))\n",
    "print('Test Max PSNR:', max(psnr_test_values))\n",
    "print('--------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86e4b9e",
   "metadata": {},
   "source": [
    "# Convergance Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dc2a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "font = {'font': 'Times New Roman', 'size': 12}\n",
    "\n",
    "plt.figure()\n",
    "axfont = {'family' : 'Times New Roman', 'weight' : 'regular', 'size'   : 10}\n",
    "plt.rc('font', **axfont)\n",
    "\n",
    "plt.plot(np.arange(len(psnr_test_values[:-1])), psnr_test_values[:-1], label = f\"{(args.inr_model).upper()}\")\n",
    "plt.xlabel('# Epochs', fontdict=font)\n",
    "plt.ylabel('PSNR (dB)', fontdict=font)\n",
    "plt.title('Image Inpainting', fontdict={'family': 'Times New Roman', 'size': 12, 'weight': 'bold'})\n",
    "plt.legend()\n",
    "plt.grid(True, color='lightgray')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
